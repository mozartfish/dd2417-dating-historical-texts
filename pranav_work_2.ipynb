{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8067c989",
   "metadata": {},
   "source": [
    "# DD2417 Final Project - Dating Historical Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74981f",
   "metadata": {},
   "source": [
    "## Libraries + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv \n",
    "import random \n",
    "import re \n",
    "import string \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed all experiments and setup \n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081e4ec",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7535c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "raw_dataset_path = './Datasets/raw_data'\n",
    "clean_dataset_path = './Datasets/clean_data'\n",
    "model_dataset_path = './Datasets/model_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c982361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all the data files in the raw data file \n",
    "print(f\"count the number of books in each decade directory in the raw data\")\n",
    "total_books = 0\n",
    "for decade in range(1700, 1900, 10):\n",
    "    decade_path = f\"{raw_dataset_path}/{decade}\"\n",
    "    if os.path.exists(decade_path):\n",
    "        text_files = [f for f in os.listdir(decade_path) if f.endswith(\".txt\")]\n",
    "        print(f\"{decade}: {len(text_files)} books\")\n",
    "        total_books += len(text_files)\n",
    "print(f\"total number of books for project: {total_books}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the titles of the books\n",
    "def get_book_titles():\n",
    "    book_titles = {}\n",
    "    for year in range(1700, 1900, 10):\n",
    "        decade_path = f\"{raw_dataset_path}/{year}\"\n",
    "        book_titles[year] = []\n",
    "\n",
    "        print(f\"decade: {year}\")\n",
    "        text_files = sorted([f for f in os.listdir(decade_path) if f.endswith('.txt')])\n",
    "        for filename in text_files:\n",
    "            file_path = os.path.join(decade_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            title_match = re.search(r\"^Title:\\s*(.+)$\", text, re.MULTILINE)\n",
    "            book_title = title_match.group(1).strip()\n",
    "            print(f\"book_title: {book_title}\")\n",
    "            book_titles[year].append(book_title)\n",
    "        print(f\"number of titles in decade: {year} -> {len(book_titles[year])}\")\n",
    "        print()\n",
    "\n",
    "    return book_titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078950aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all old data files\n",
    "if os.path.exists(clean_dataset_path):\n",
    "    print(f\"clean up - previous clean_data_files\")\n",
    "    directories = os.listdir(clean_dataset_path)\n",
    "    directories.sort()\n",
    "    for dir in directories:\n",
    "        decade_path = os.path.join(clean_dataset_path, dir)\n",
    "        if os.path.isdir(decade_path):\n",
    "            text_files = os.listdir(decade_path)\n",
    "            text_files.sort()\n",
    "            for file in text_files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(decade_path, file)\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"succesfully removed {file}\")\n",
    "            os.rmdir(decade_path)\n",
    "            print(f\"successfully removed directory {dir}\")\n",
    "            print()\n",
    "    os.rmdir(clean_dataset_path)\n",
    "    print(f\"succesfully removed {clean_dataset_path}\")\n",
    "    print()\n",
    "\n",
    "# create new data files\n",
    "if os.path.exists(model_dataset_path):\n",
    "    print(f\"clean up - previous model data\")\n",
    "    data_files = [f for f in os.listdir(model_dataset_path) if f.endswith(\".csv\")]\n",
    "    for file in data_files:\n",
    "        file_path = os.path.join(model_dataset_path, file)\n",
    "        os.remove(file_path)\n",
    "        print(f\"succesfully removed {file}\")\n",
    "    os.rmdir(model_dataset_path)\n",
    "    print(f\"succesfully removed {model_dataset_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97805283",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(clean_dataset_path):\n",
    "    print(f\"create clean data directory\")\n",
    "    os.makedirs(clean_dataset_path)\n",
    "\n",
    "if not os.path.exists(model_dataset_path):\n",
    "    print(f\"create model data directory for storing data for building models\")\n",
    "    os.makedirs(model_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66beed86",
   "metadata": {},
   "source": [
    "### Data-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove everything up to and including start\n",
    "    start_match = re.search(\n",
    "        r\"\\*\\*\\* START OF.*?\\*\\*\\*\", text, re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if start_match:\n",
    "        text = text[start_match.end():]\n",
    "    \n",
    "    # remove everything after end \n",
    "    end_match = re.search(\n",
    "        r'\\*\\*\\* END OF.*?\\*\\*\\*', text, re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if end_match:\n",
    "        text = text[:end_match.start()]\n",
    "    \n",
    "    # remove years \n",
    "    text = re.sub(r'\\b1[0-9]{3}\\b', '', text)\n",
    "\n",
    "    # remove whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52824720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(dataset_path, year):\n",
    "    print(f\"preprocess text\")\n",
    "    print(f\"directory year: {year}\")\n",
    "    decade_path = dataset_path + \"/\" + str(year) + \"/\"\n",
    "    cleaned_data_path = f'{clean_dataset_path}/{year}/'\n",
    "    if not os.path.exists(cleaned_data_path):\n",
    "        os.makedirs(cleaned_data_path)\n",
    "    text_list = os.listdir(decade_path)\n",
    "    # print(f\"text files: {text_list}\")\n",
    "    for text_file in text_list:\n",
    "        if text_file.endswith('.txt'):\n",
    "            print(f\"file name: {text_file}\")\n",
    "            # read file \n",
    "            with open(decade_path + text_file, 'r', encoding='utf-8') as f:\n",
    "                raw_text = f.read()\n",
    "                print(f\"read file sucessfully\")\n",
    "\n",
    "            cleaned_text = clean_text(raw_text)\n",
    "            print(f\"text cleaned succesfully\")\n",
    "\n",
    "            # save file \n",
    "            out_file = cleaned_data_path + text_file\n",
    "            with open(out_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "                print(f\"cleaned text succesfully saved to {out_file}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343480d",
   "metadata": {},
   "source": [
    "### 1700s Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9269480",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [1700, 1710, 1720, 1730, 1740, 1750, 1760, 1770, 1780, 1790]\n",
    "for year in years:\n",
    "    preprocess_text(raw_dataset_path, year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4872dc9",
   "metadata": {},
   "source": [
    "### 1800's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ace257",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [1800, 1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890]\n",
    "for year in years:\n",
    "    preprocess_text(raw_dataset_path, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555059ff",
   "metadata": {},
   "source": [
    "### Dataset Splits - Train, Validation, Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info():\n",
    "    years = [i for i in range(1700, 1900, 10)]\n",
    "    book_titles = get_book_titles()\n",
    "\n",
    "    book_data = [] \n",
    "    for decade in years:\n",
    "        decade_path = f'{clean_dataset_path}/{decade}'\n",
    "        if os.path.exists(decade_path):\n",
    "            text_files = sorted(\n",
    "                [f for f in os.listdir(decade_path) if f.endswith(\".txt\")]\n",
    "            )\n",
    "            for index, filename in enumerate(text_files):\n",
    "                if decade in book_titles and index < len(book_titles[decade]):\n",
    "                    book_title = book_titles[decade][index]\n",
    "                else: \n",
    "                    book_title = f\"unknown_book_{index + 1}\"\n",
    "                book_info = {\n",
    "                'decade': decade,\n",
    "                'filename': filename,\n",
    "                'book_title': book_title,\n",
    "                'filepath': os.path.join(decade_path, filename),\n",
    "                'book_id': f\"{decade}_{book_title[:20].replace(' ', '_')}\" \n",
    "            }\n",
    "                book_data.append(book_info)\n",
    "    return book_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_splits(book_data):\n",
    "    train_books, valid_books, test_books = [], [], []\n",
    "\n",
    "    # group books by decade \n",
    "    books_by_decade = {}\n",
    "    for book in book_data:\n",
    "        decade = book['decade']\n",
    "        if decade not in books_by_decade:\n",
    "            books_by_decade[decade] = []\n",
    "        books_by_decade[decade].append(book)\n",
    "    \n",
    "    for decade, books in books_by_decade.items():\n",
    "        num_books = len(books)\n",
    "        if num_books == 4:\n",
    "            train_books.extend(books[:2])\n",
    "            valid_books.extend(books[2:3])\n",
    "            test_books.extend(books[3:4])\n",
    "\n",
    "        elif num_books == 5:\n",
    "            train_books.extend(books[:3])\n",
    "            valid_books.extend(books[3:4])\n",
    "            test_books.extend(books[4:5])\n",
    "\n",
    "        elif num_books == 6:\n",
    "            train_books.extend(books[:4])\n",
    "            valid_books.extend(books[4:5])\n",
    "            test_books.extend(books[5:6])\n",
    "    \n",
    "    print(f\"train data: {train_books}\")\n",
    "    print(f\"valid data: {valid_books}\")\n",
    "    print(f\"test data: {test_books}\")\n",
    "    print()\n",
    "\n",
    "    return train_books, valid_books, test_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = dataset_info()\n",
    "train_data, valid_data, test_data = create_dataset_splits(book_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8dcafd",
   "metadata": {},
   "source": [
    "### Random Sampling Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51643e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_passages(book_list, num_passages=5, passage_length=1500):\n",
    "    passages_data = []\n",
    "    for book in book_list:\n",
    "        with open(book['filepath'], 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # randomly sample passages \n",
    "        for i in range(num_passages):\n",
    "            max_start = len(text) - passage_length\n",
    "            start_pos = random.randint(0, max_start)\n",
    "            passage = text[start_pos:start_pos + passage_length]\n",
    "            passage_info = {\n",
    "                'text': passage,\n",
    "                'decade': book['decade'],\n",
    "                'decade_id': (book['decade'] - 1700) // 10,\n",
    "                'book_title': book['book_title'],\n",
    "                'book_id': book['book_id'],\n",
    "                'passage_id': f\"{book['book_id']}_passage_{i}\"\n",
    "            }\n",
    "            passages_data.append(passage_info)\n",
    "    \n",
    "    return passages_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_csv(passages_data, filepath):\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # write header\n",
    "        writer.writerow(['text', \n",
    "                         'decade',\n",
    "                         'book_title', \n",
    "                         'passage_id',\n",
    "                        'decade_id', \n",
    "                         'book_id'])\n",
    "\n",
    "        for passage in passages_data:\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    passage['text'].replace(\"\\n\", \" \").replace(\"\\r\", \" \"),\n",
    "                    passage['decade'],\n",
    "                    passage['book_title'],\n",
    "                    passage['passage_id'],\n",
    "                    passage['decade_id'],\n",
    "                    passage['book_id']\n",
    "                ]\n",
    "            )\n",
    "        print(f\"Save {len(passages_data)} passages to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d540b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"get book data and create splits\")\n",
    "book_data = dataset_info()\n",
    "train_books, validation_books, test_books = create_dataset_splits(book_data)\n",
    "print(f\"Book splits: {len(train_books)} -> training_data, {len(validation_books)} -> validation_data, {len(test_books)} -> test_data\")\n",
    "print()\n",
    "print(f\"Randomly sample passages for creating datasets\")\n",
    "train_passages = random_sampling_passages(train_books, num_passages=20, passage_length=1500)\n",
    "validation_passages = random_sampling_passages(validation_books, num_passages=15, passage_length=1500)  \n",
    "test_passages = random_sampling_passages(test_books, num_passages=10, passage_length=1500)\n",
    "print(f\"Passage counts: {len(train_passages)} -> training_passages, {len(validation_passages)} -> validation_passages, {len(test_passages)} -> test_passages\")\n",
    "print()\n",
    "print(f\"write data to csv files\")\n",
    "write_data_to_csv(train_passages, f\"{model_dataset_path}/train_passages.csv\")\n",
    "write_data_to_csv(validation_passages, f\"{model_dataset_path}/validation_passages.csv\")\n",
    "write_data_to_csv(test_passages, f\"{model_dataset_path}/test_passages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05070628",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ff93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6253957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
